{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting flight delay\n",
    "\n",
    "We are going to make models that predict wheter a flight will be delayed:\n",
    "\n",
    "We will first explore the flight data, then clean it and lastly build a model that predicts if a flight will be delayed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we initialize spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "### Specify clusers. The name. Get or create will make sure that we do not initialize two times the same session \n",
    "spark = SparkSession.builder.master('local[*]').appName('flights delay').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Next we read the data for this we use spark.read.csv\n",
    "flights = spark.read.csv('./data/flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True, ### slow -> must go true the entire data once. We can specify the schema.\n",
    "                         nullValue=\"NA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
      "|  5|  2|  1|     UA|   704|SFO| 550|  7.98|     102|    2|\n",
      "|  7|  2|  6|     AA|   380|ORD| 733| 10.83|     135|   54|\n",
      "|  1| 16|  6|     UA|  1477|ORD|1440|   8.0|     232|   -7|\n",
      "|  1| 22|  5|     UA|   620|SJC|1829|  7.98|     250|  -13|\n",
      "| 11|  8|  1|     OO|  5590|SFO| 158|  7.77|      60|   88|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### We can view the first 10 entries (notis the null values):\n",
    "flights.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mon', 'int'),\n",
       " ('dom', 'int'),\n",
       " ('dow', 'int'),\n",
       " ('carrier', 'string'),\n",
       " ('flight', 'int'),\n",
       " ('org', 'string'),\n",
       " ('mile', 'int'),\n",
       " ('depart', 'double'),\n",
       " ('duration', 'int'),\n",
       " ('delay', 'int')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We can also check the dtype\n",
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2978\n"
     ]
    }
   ],
   "source": [
    "### We next check how many flights have a missing delay value, we can do this with the filter command:\n",
    "# Number of records with missing 'delay' values\n",
    "print(flights.filter('delay IS NULL').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47022\n",
      "47022\n"
     ]
    }
   ],
   "source": [
    "### With the use of the filter we can create a new dataframe\n",
    "flights_valid_delay = flights.filter('delay IS NOT NULL')\n",
    "### We check how many are left\n",
    "print(flights_valid_delay.count())\n",
    "\n",
    "\n",
    "# We can also do it with the dropna (drops every row with NA values)\n",
    "flights_na_cleaned = flights.dropna()\n",
    "print(flights_na_cleaned.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we use drop the \"flight\" column as it does not provide us any information\n",
    "flights_cleaned = flights_na_cleaned.drop(\"flight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|\n",
      "+---+---+---+-------+---+------+--------+-----+------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "### Now we show how to create new columns\n",
    "# We do not like the imperial system hence we will create a km column and remove the mile column\n",
    "flights_km = flights_cleaned.withColumn(\"km\", round(flights.mile *1.60934, 0 )).drop('mile')\n",
    "\n",
    "flights_km.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Next we want to see if a flight is delayed or not. A flight is officially delayed when it arives 15 minutes or later\n",
    "### We again create a new column\n",
    "flights_delayed = flights_km.withColumn(\"label\", (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# Next we check the results\n",
    "flights_delayed.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|\n",
      "|  1| 16|  6|     UA|ORD|   8.0|     232|   -7|2317.0|    0|        0.0|    0.0|\n",
      "|  1| 22|  5|     UA|SJC|  7.98|     250|  -13|2943.0|    0|        0.0|    5.0|\n",
      "| 11|  8|  1|     OO|SFO|  7.77|      60|   88| 254.0|    1|        2.0|    1.0|\n",
      "|  4| 26|  1|     AA|SFO| 13.25|     210|  -10|2356.0|    0|        1.0|    1.0|\n",
      "|  4| 25|  0|     AA|ORD| 13.75|     160|   31|1574.0|    1|        1.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Creating an indexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Creating the object, describing the input collumn and the output column\n",
    "indexer = StringIndexer(inputCol=\"carrier\", outputCol='carrier_idx')\n",
    "\n",
    "# The indecer needs to be fit on the data\n",
    "indexer_model = indexer.fit(flights_delayed)\n",
    "\n",
    "# Then we need to transform the data. \n",
    "flights_indexed = indexer_model.transform(flights_delayed)\n",
    "\n",
    "# A one liner for the org column\n",
    "flights_indexed = StringIndexer(inputCol=\"org\", outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
    "flights_indexed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|  carrier_hot|      org_hot|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|(8,[0],[1.0])|(7,[0],[1.0])|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|(8,[0],[1.0])|(7,[1],[1.0])|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|(8,[1],[1.0])|(7,[0],[1.0])|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|(8,[0],[1.0])|(7,[1],[1.0])|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|(8,[1],[1.0])|(7,[0],[1.0])|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Indexes are not good for classification as indexes do not have a linear ordering. Hence we will create a one-hot encoding\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "one_hot = OneHotEncoder(inputCols=[\"carrier_idx\", \"org_idx\"], outputCols=[\"carrier_hot\", \"org_hot\"])\n",
    "one_hot_model = one_hot.fit(flights_indexed)\n",
    "flights_hot = one_hot_model.transform(flights_indexed)\n",
    "\n",
    "print(flights_hot.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+-----+\n",
      "|features                                                            |delay|\n",
      "+--------------------------------------------------------------------+-----+\n",
      "|(21,[1,2,3,11,18,19,20],[22.0,2.0,1.0,1.0,509.0,16.33,82.0])        |30   |\n",
      "|(21,[0,1,2,3,12,18,19,20],[2.0,20.0,4.0,1.0,1.0,542.0,6.17,82.0])   |-8   |\n",
      "|(21,[0,1,2,4,11,18,19,20],[9.0,13.0,1.0,1.0,1.0,1989.0,10.33,195.0])|-5   |\n",
      "|(21,[0,1,2,3,12,18,19,20],[5.0,2.0,1.0,1.0,1.0,885.0,7.98,102.0])   |2    |\n",
      "|(21,[0,1,2,4,11,18,19,20],[7.0,2.0,6.0,1.0,1.0,1180.0,10.83,135.0]) |54   |\n",
      "+--------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### lastly, in pyspark we need to assemble all columns that we want to use for predictions into 1 columns. We will use the Vector Assembler for this\n",
    "\n",
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_hot', 'org_hot', 'km', 'depart','duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_hot)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25475650433622415\n"
     ]
    }
   ],
   "source": [
    "### Now that we cleaned the data, we will start the training\n",
    "## First create a random split\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], 17) ## 17 = seed\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_test.count() / flights_train.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------------------+\n",
      "|label|prediction|probability                             |\n",
      "+-----+----------+----------------------------------------+\n",
      "|1    |0.0       |[0.5996802923041791,0.400319707695821]  |\n",
      "|0    |1.0       |[0.33513097072419107,0.6648690292758089]|\n",
      "|0    |0.0       |[0.5996802923041791,0.400319707695821]  |\n",
      "|1    |1.0       |[0.33513097072419107,0.6648690292758089]|\n",
      "|1    |1.0       |[0.3125577100646353,0.6874422899353647] |\n",
      "|1    |1.0       |[0.3125577100646353,0.6874422899353647] |\n",
      "|1    |1.0       |[0.33513097072419107,0.6648690292758089]|\n",
      "|0    |1.0       |[0.33513097072419107,0.6648690292758089]|\n",
      "|1    |1.0       |[0.33513097072419107,0.6648690292758089]|\n",
      "|1    |1.0       |[0.33513097072419107,0.6648690292758089]|\n",
      "+-----+----------+----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Next we will use a decision tree:\n",
    "# Import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1333|\n",
      "|    0|       0.0| 2303|\n",
      "|    1|       1.0| 3577|\n",
      "|    0|       1.0| 2334|\n",
      "+-----+----------+-----+\n",
      "\n",
      "TN 2303\n",
      "TP 3577\n",
      "FN 1333\n",
      "FP 2334\n",
      "0.6159002828113543\n"
     ]
    }
   ],
   "source": [
    "### Confusion matrix\n",
    "# Create a confusion matrix\n",
    "prediction.groupBy(\"label\", 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP)/ (TN + TP + FN + FP)\n",
    "print(\"TN\", TN)\n",
    "print(\"TP\", TP)\n",
    "print(\"FN\", FN)\n",
    "print(\"FP\", FP)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1618|\n",
      "|    0|       0.0| 2498|\n",
      "|    1|       1.0| 3292|\n",
      "|    0|       1.0| 2139|\n",
      "+-----+----------+-----+\n",
      "\n",
      "TN 2498\n",
      "TP 3292\n",
      "FN 1618\n",
      "FP 2139\n",
      "0.6064732376662826\n"
     ]
    }
   ],
   "source": [
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP)/ (TN + TP + FN + FP)\n",
    "print(\"TN\", TN)\n",
    "print(\"TP\", TP)\n",
    "print(\"FN\", FN)\n",
    "print(\"FP\", FP)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0573, 0.0003, -0.0043, 0.7835, 0.924, 0.7461, 0.601, 0.6403, 0.9975, 0.3143, 0.5775, 1.3961, 1.2412, 1.2774, 1.3595, 0.7784, 0.6897, 0.7251, -0.0, 0.0789, 0.0017])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We can check the coefficients\n",
    "logistic.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1517|\n",
      "|    0|       0.0| 2403|\n",
      "|    1|       1.0| 3393|\n",
      "|    0|       1.0| 2234|\n",
      "+-----+----------+-----+\n",
      "\n",
      "TN 2403\n",
      "TP 3393\n",
      "FN 1517\n",
      "FP 2234\n",
      "0.6071017073426207\n"
     ]
    }
   ],
   "source": [
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Create a classifier object and train on training data\n",
    "svc = LinearSVC().fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = svc.transform(flights_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP)/ (TN + TP + FN + FP)\n",
    "print(\"TN\", TN)\n",
    "print(\"TP\", TP)\n",
    "print(\"FN\", FN)\n",
    "print(\"FP\", FP)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6078545760403053\n",
      "0.6452004091766956\n"
     ]
    }
   ],
   "source": [
    "### evaluating\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "print(weighted_precision)\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
