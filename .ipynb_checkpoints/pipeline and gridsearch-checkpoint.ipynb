{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pipelines\n",
    "\n",
    "Instead of doing each step manually, we can also define a pipeline. This streamlines the process and makes it easy to inspect which steps the data has gone through.\n",
    "\n",
    "Furthermore we will show how to use gridsearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pyspark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('pipeline').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "### Read the data\n",
    "flights = spark.read.csv('./data/flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue=\"NA\")\n",
    "\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], 17) ## 17 = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
      "|  5|  2|  1|     UA|   704|SFO| 550|  7.98|     102|    2|\n",
      "|  7|  2|  6|     AA|   380|ORD| 733| 10.83|     135|   54|\n",
      "|  1| 16|  6|     UA|  1477|ORD|1440|   8.0|     232|   -7|\n",
      "|  1| 22|  5|     UA|   620|SJC|1829|  7.98|     250|  -13|\n",
      "| 11|  8|  1|     OO|  5590|SFO| 158|  7.77|      60|   88|\n",
      "|  4| 26|  1|     AA|  1144|SFO|1464| 13.25|     210|  -10|\n",
      "|  4| 25|  0|     AA|   321|ORD| 978| 13.75|     160|   31|\n",
      "|  8| 30|  2|     UA|   646|ORD| 719| 13.28|     151|   16|\n",
      "|  3| 16|  3|     UA|   107|ORD|1745|   9.0|     264|    3|\n",
      "|  0|  3|  4|     AA|  1559|LGA|1097| 17.08|     190|   32|\n",
      "|  5|  9|  1|     UA|   770|SFO| 967|  12.7|     158|   20|\n",
      "|  3| 10|  4|     B6|   937|ORD|1735| 17.58|     265|  155|\n",
      "| 11| 15|  1|     AA|  2303|ORD| 802|  6.75|     160|   23|\n",
      "|  8| 18|  4|     UA|   802|SJC| 948|  6.33|     160|   17|\n",
      "|  2| 14|  5|     B6|    71|JFK| 944|  6.17|     166|    0|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we show that we can create a custom transfomer\n",
    "# We already have seen some transformers, such as StringIndexer, OneHotEncoder and VectorAssembler.\n",
    "# But we can create our own!\n",
    "\n",
    "from pyspark.ml.pipeline import Transformer\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "class MileToKm(Transformer):\n",
    "\n",
    "    def __init__(self, inputCol='mile', outputCol='km'):\n",
    "        super(MileToKm, self).__init__()\n",
    "        self.inputCol = inputCol #the name of your columns\n",
    "        self.outputCol = outputCol #the name of your output column\n",
    "\n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        #assert that field is a datetype \n",
    "        if (field.dataType != IntegerType()):\n",
    "            raise Exception('MileToKm input type %s did not match input type IntType' % field.dataType)\n",
    "            \n",
    "    ## Need to define this function. (Can also define a fit function)\n",
    "    def _transform(self, df):\n",
    "        self.check_input_type(df.schema)\n",
    "        return df.withColumn(self.outputCol, F.round(df[self.inputCol] * 1.60934, 0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# Convert categorical strings to index values\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "km_transform = MileToKm()\n",
    "# One-hot encode index values\n",
    "onehot = OneHotEncoder(\n",
    "    inputCols=['org_idx', 'dow'],\n",
    "    outputCols=['org_one_hot', 'dow_one_hot']\n",
    ")\n",
    "\n",
    "# Assemble predictors into a single column\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_one_hot', 'dow_one_hot'], outputCol='features')\n",
    "\n",
    "# A linear regression object\n",
    "regression = LinearRegression(labelCol='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+------+-------+-------------+-------------+--------------------+------------------+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|    km|org_idx|  org_one_hot|  dow_one_hot|            features|        prediction|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+------+-------+-------------+-------------+--------------------+------------------+\n",
      "|  0|  1|  2|     AA|     3|JFK|2475|  12.0|     370|   11|3983.0|    2.0|(7,[2],[1.0])|(6,[2],[1.0])|(14,[0,3,10],[398...|364.38375267209113|\n",
      "|  0|  1|  2|     AA|   254|OGG|2486| 15.33|     310|  173|4001.0|    7.0|    (7,[],[])|(6,[2],[1.0])|(14,[0,10],[4001....| 313.2461800798374|\n",
      "|  0|  1|  2|     AA|   336|ORD| 733| 21.58|     115|   55|1180.0|    0.0|(7,[0],[1.0])|(6,[2],[1.0])|(14,[0,1,10],[118...| 131.7211766912025|\n",
      "|  0|  1|  2|     AA|   678|SFO| 337| 16.25|      80|  139| 542.0|    1.0|(7,[1],[1.0])|(6,[2],[1.0])|(14,[0,2,10],[542...| 76.35027806659579|\n",
      "|  0|  1|  2|     AA|   705|LGA|1389|   6.5|     240|   40|2235.0|    3.0|(7,[3],[1.0])|(6,[2],[1.0])|(14,[0,4,10],[223...|228.32755792389906|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+------+-------+-------------+-------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import class for creating a pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Construct a pipeline\n",
    "pipeline_regression = Pipeline(stages=[km_transform, indexer, onehot, assembler, regression])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline_regression = pipeline_regression.fit(flights_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = pipeline_regression.transform(flights_test)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.163418774419267"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Next we will use a cross validation and grid search to find the best parameters of our model\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "# Create an empty parameter grid\n",
    "params = ParamGridBuilder()\n",
    "params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
    "               .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "params = params.build()\n",
    "\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='duration')\n",
    "\n",
    "# Construct a pipeline\n",
    "pipeline_regression = Pipeline(stages=[km_transform, indexer, onehot, assembler, regression])\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline_regression, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train and test model on multiple folds of the training data\n",
    "cv = cv.fit(flights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.163705452721718"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the predictions. cv.transform uses the best model it found\n",
    "predictions = cv.transform(flights_test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+------+-------+-------------+-------------+--------------------+------------------+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|    km|org_idx|  org_one_hot|  dow_one_hot|            features|        prediction|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+------+-------+-------------+-------------+--------------------+------------------+\n",
      "|  0|  1|  2|     AA|     3|JFK|2475|  12.0|     370|   11|3983.0|    2.0|(7,[2],[1.0])|(6,[2],[1.0])|(14,[0,3,10],[398...|364.35938675547595|\n",
      "|  0|  1|  2|     AA|   254|OGG|2486| 15.33|     310|  173|4001.0|    7.0|    (7,[],[])|(6,[2],[1.0])|(14,[0,10],[4001....|313.35805172817106|\n",
      "|  0|  1|  2|     AA|   336|ORD| 733| 21.58|     115|   55|1180.0|    0.0|(7,[0],[1.0])|(6,[2],[1.0])|(14,[0,1,10],[118...|131.72115830804273|\n",
      "|  0|  1|  2|     AA|   678|SFO| 337| 16.25|      80|  139| 542.0|    1.0|(7,[1],[1.0])|(6,[2],[1.0])|(14,[0,2,10],[542...|  76.3575801752061|\n",
      "|  0|  1|  2|     AA|   705|LGA|1389|   6.5|     240|   40|2235.0|    3.0|(7,[3],[1.0])|(6,[2],[1.0])|(14,[0,4,10],[223...|228.31360216220227|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+------+-------+-------------+-------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineModel_3744c088514a\n",
      "[MileToKm_b98e3249739c, StringIndexerModel: uid=StringIndexer_6918cc853d66, handleInvalid=error, OneHotEncoderModel: uid=OneHotEncoder_97d53c79f3bb, dropLast=true, handleInvalid=error, numInputCols=2, numOutputCols=2, VectorAssembler_86d2f4b508b2, LinearRegressionModel: uid=LinearRegression_98338991af29, numFeatures=14]\n"
     ]
    }
   ],
   "source": [
    "# We can inspect the best model and see which stages where used\n",
    "print(cv.bestModel)\n",
    "print(cv.bestModel.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chceck which params where used\n",
    "params = cv.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression_98338991af29__aggregationDepth\n",
      "2\n",
      "LinearRegression_98338991af29__elasticNetParam\n",
      "0.0\n",
      "LinearRegression_98338991af29__featuresCol\n",
      "features\n",
      "LinearRegression_98338991af29__fitIntercept\n",
      "True\n",
      "LinearRegression_98338991af29__labelCol\n",
      "duration\n",
      "LinearRegression_98338991af29__predictionCol\n",
      "prediction\n",
      "LinearRegression_98338991af29__solver\n",
      "auto\n",
      "LinearRegression_98338991af29__standardization\n",
      "True\n",
      "LinearRegression_98338991af29__epsilon\n",
      "1.35\n",
      "LinearRegression_98338991af29__loss\n",
      "squaredError\n",
      "LinearRegression_98338991af29__maxIter\n",
      "100\n",
      "LinearRegression_98338991af29__regParam\n",
      "0.01\n",
      "LinearRegression_98338991af29__tol\n",
      "1e-06\n"
     ]
    }
   ],
   "source": [
    "for key, value in params.items():\n",
    "    print(key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0743, 28.0131, 20.0642, 52.3391, 46.2005, 15.2115, 17.8247, 17.1911, 0.4594, 0.0933, -0.1281, 0.2236, 0.2601, 0.1102])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can again check the coefficients\n",
    "cv.bestModel.stages[-1].coefficients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
